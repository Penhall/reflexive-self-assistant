#!/usr/bin/env python3
"""
Teste r√°pido para verificar se qwen2:1.5b est√° funcionando
"""

import sys
import time
from pathlib import Path

# Adicionar path do projeto
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_qwen_setup():
    """Teste completo do setup com qwen2:1.5b"""
    
    print("üß™ TESTE R√ÅPIDO: qwen2:1.5b Setup")
    print("=" * 50)
    
    # 1. Testar importa√ß√µes
    print("\n1Ô∏è‚É£ Testando importa√ß√µes...")
    try:
        from core.llm.llm_manager import llm_manager
        print("‚úÖ LLM Manager importado")
    except Exception as e:
        print(f"‚ùå Erro de importa√ß√£o: {e}")
        return False
    
    # 2. Verificar status do sistema
    print("\n2Ô∏è‚É£ Verificando status do sistema...")
    try:
        status = llm_manager.get_system_status()
        print(f"‚úÖ Status: {status['ready']}")
        print(f"   LLM Real: {status['using_real_llm']}")
        print(f"   Host: {status['host']}")
        print(f"   Modelos: {status['models_configured']}")
    except Exception as e:
        print(f"‚ùå Erro no status: {e}")
        return False
    
    # 3. Testar disponibilidade do Ollama
    print("\n3Ô∏è‚É£ Testando Ollama...")
    try:
        available = llm_manager.is_available()
        models = llm_manager.list_models()
        print(f"‚úÖ Ollama dispon√≠vel: {available}")
        print(f"   Modelos instalados: {len(models)}")
        for model in models:
            print(f"   ‚Ä¢ {model}")
            
        if "qwen2:1.5b" not in models and available:
            print("üì• qwen2:1.5b n√£o encontrado. Instalando...")
            success = llm_manager.pull_model("qwen2:1.5b")
            print(f"   Instala√ß√£o: {'‚úÖ' if success else '‚ùå'}")
    except Exception as e:
        print(f"‚ùå Erro no Ollama: {e}")
    
    # 4. Teste de gera√ß√£o simples
    print("\n4Ô∏è‚É£ Testando gera√ß√£o de c√≥digo...")
    try:
        start_time = time.time()
        response = llm_manager.generate_code("criar fun√ß√£o que soma dois n√∫meros")
        end_time = time.time()
        
        print(f"‚úÖ Gera√ß√£o conclu√≠da em {end_time - start_time:.2f}s")
        print(f"   Modelo usado: {response.model}")
        print(f"   Sucesso: {response.success}")
        print(f"   Tokens: {response.context_tokens} + {response.response_tokens} = {response.tokens_used}")
        print(f"   Tempo de gera√ß√£o: {response.generation_time:.2f}s")
        
        # Mostrar preview do c√≥digo
        preview = response.content[:150].replace('\n', '\\n')
        print(f"   Preview: {preview}...")
        
        # Verificar se √© mock ou real
        if "mock" in response.model.lower():
            print("‚ö†Ô∏è  Usando modo mock (Ollama pode estar offline)")
        else:
            print("üöÄ Usando LLM real!")
            
    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o: {e}")
        return False
    
    # 5. Teste de mem√≥ria/GraphRAG (opcional)
    print("\n5Ô∏è‚É£ Testando integra√ß√£o GraphRAG...")
    try:
        from memory.hybrid_store import HybridMemoryStore
        memory = HybridMemoryStore(enable_graphrag=True)
        print("‚úÖ HybridMemoryStore inicializado")
        memory.close()
    except Exception as e:
        print(f"‚ö†Ô∏è GraphRAG n√£o dispon√≠vel: {e}")
    
    # 6. Resumo final
    print("\n" + "=" * 50)
    if llm_manager.is_mock:
        print("üìã RESULTADO: Sistema funcionando em MODO MOCK")
        print("üí° Para usar LLM real:")
        print("   1. Verifique se Ollama est√° rodando: ollama serve")
        print("   2. Instale qwen2:1.5b: ollama pull qwen2:1.5b")
    else:
        print("üìã RESULTADO: Sistema funcionando com LLM REAL! üöÄ")
        print(f"   Modelo: {response.model}")
        print(f"   Performance: {response.generation_time:.2f}s")
    
    print("‚úÖ Setup conclu√≠do!")
    return True

if __name__ == "__main__":
    success = test_qwen_setup()
    sys.exit(0 if success else 1)