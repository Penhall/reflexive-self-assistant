#!/usr/bin/env python3
"""
Teste √∫nico e completo do sistema RSCA
Substitui todos os outros arquivos de teste
"""

import sys
import time
import requests
from pathlib import Path
from datetime import datetime

# Adicionar path do projeto
sys.path.append(str(Path(__file__).parent.parent.parent))

class RSCASystemTest:
    def __init__(self):
        self.results = {}
        self.start_time = time.time()
        # Definir modelos a serem usados nos testes
        self.ollama_code_model = "deepseek-r1:1.5b"
        self.ollama_general_model = "qwen2:1.5b"
    
    def test_ollama_connection(self):
        """Testa conex√£o com Ollama e verifica se os modelos necess√°rios est√£o presentes."""
        print("üîó Testando conex√£o Ollama...")
        
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json().get("models", [])
                model_names = [m["name"] for m in models_data]
                
                print(f"‚úÖ Ollama funcionando! Modelos dispon√≠veis: {len(model_names)}")
                for model in model_names:
                    print(f"   ‚Ä¢ {model}")
                
                # Verificar se os modelos espec√≠ficos est√£o presentes
                has_code_model = any(m.startswith(self.ollama_code_model.split(':')[0]) for m in model_names)
                has_general_model = any(m.startswith(self.ollama_general_model.split(':')[0]) for m in model_names)

                if has_code_model and has_general_model:
                    print(f"‚úÖ Modelos '{self.ollama_code_model}' e '{self.ollama_general_model}' encontrados.")
                    return True, model_names
                else:
                    print(f"‚ùå Modelos '{self.ollama_code_model}' ou '{self.ollama_general_model}' n√£o encontrados.")
                    return False, model_names
                
            else:
                print(f"‚ùå Ollama retornou status {response.status_code}")
                return False, []
                
        except Exception as e:
            print(f"‚ùå Erro na conex√£o: {e}")
            return False, []
    
    def test_ollama_generation(self, model_to_use):
        """Testa gera√ß√£o com Ollama usando um modelo espec√≠fico."""
        print(f"\nü§ñ Testando gera√ß√£o de c√≥digo com modelo: {model_to_use}...")
        
        try:
            payload = {
                "model": model_to_use,
                "prompt": "def calculate_sum(a, b): return",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 100
                }
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "").strip()
                
                if content:
                    print("‚úÖ Gera√ß√£o funcionando!")
                    print(f"üìÑ C√≥digo gerado:")
                    print("-" * 30)
                    print(content[:150] + "..." if len(content) > 150 else content)
                    print("-" * 30)
                    return True
                else:
                    print("‚ö†Ô∏è Resposta vazia")
                    return False
            else:
                print(f"‚ùå Erro HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"‚ùå Erro na gera√ß√£o: {e}")
            return False
    
    def test_code_agent(self):
        """Testa CodeAgent."""
        print("\nüõ†Ô∏è Testando CodeAgent...")
        
        try:
            from core.agents.code_agent import CodeAgent
            
            # Criar agente, garantindo que use o modelo correto
            # O CodeAgent deve pegar o modelo de config.settings.py
            agent = CodeAgent(use_mock=False)
            
            # Tarefa simples
            task = "Criar fun√ß√£o que multiplica dois n√∫meros"
            print(f"üìù Tarefa: {task}")
            
            result = agent.execute_task(task)
            
            print(f"üìä Resultado:")
            print(f"   Sucesso: {result.success}")
            print(f"   Qualidade: {result.quality_score:.1f}/10")
            print(f"   Modelo: {getattr(result, 'llm_used', 'unknown')}")
            
            if result.success:
                print(f"‚úÖ CodeAgent funcionando!")
                if hasattr(result, 'execution_result') and result.execution_result:
                    print(f"   Execu√ß√£o: {result.execution_result.strip()}")
                return True
            else:
                print(f"‚ùå CodeAgent falhou: {getattr(result, 'error', 'Erro desconhecido')}")
                return False
                
        except ImportError as e:
            print(f"‚ùå Erro de import: {e}")
            print("üí° Execute: python scripts/fixes/fix_imports.py")
            return False
        except Exception as e:
            print(f"‚ùå Erro no CodeAgent: {e}")
            return False
    
    def test_pipeline(self):
        """Testa pipeline completo."""
        print("\nüîÑ Testando pipeline Code->Test->Doc...")
        
        try:
            from core.agents.code_agent import CodeAgent
            from core.agents.test_agent import TestAgent
            from core.agents.doc_agent import DocumentationAgent
            
            # Criar agentes
            code_agent = CodeAgent(use_mock=False)
            test_agent = TestAgent()
            doc_agent = DocumentationAgent()
            
            task = "Fun√ß√£o que calcula √°rea de um c√≠rculo"
            print(f"üìù Tarefa: {task}")
            
            # 1. C√≥digo
            print("1Ô∏è‚É£ Gerando c√≥digo...")
            code_result = code_agent.execute_task(task)
            
            if not code_result.success:
                print("‚ùå Falha na gera√ß√£o de c√≥digo")
                return False
            
            # 2. Testes
            print("2Ô∏è‚É£ Gerando testes...")
            try:
                test_code = test_agent.generate_tests(code_result.code, "calcular_area_circulo")
                print("‚úÖ Testes gerados!")
                
                # Mostrar primeira linha do teste
                first_line = test_code.split('\n')[0] if test_code else "Vazio"
                print(f"   Primeira linha: {first_line}")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erro nos testes: {e}")
            
            # 3. Documenta√ß√£o
            print("3Ô∏è‚É£ Gerando documenta√ß√£o...")
            try:
                doc_agent.create_docs(code_result.code)
                print("‚úÖ Documenta√ß√£o gerada!")
                print(f"   Conte√∫do: {doc_agent.latest_output[:100]}...")
                
            except Exception as e:
                print(f"‚ö†Ô∏è Erro na documenta√ß√£o: {e}")
            
            print("‚úÖ Pipeline funcionando!")
            return True
            
        except ImportError as e:
            print(f"‚ùå Erro de import no pipeline: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Erro no pipeline: {e}")
            return False
    
    def test_reflection_system(self):
        """Testa sistema de reflex√£o."""
        print("\nüß† Testando sistema de reflex√£o...")
        
        try:
            from core.agents.reflection_agent import ReflectionAgent
            from core.agents.code_agent import CodeAgent
            
            # Criar agentes
            code_agent = CodeAgent(use_mock=True)  # Mock para rapidez
            reflection_agent = ReflectionAgent()
            
            # Gerar algum c√≥digo
            code_agent.execute_task("Fun√ß√£o simples de teste")
            
            # Testar reflex√£o
            print("üîç Executando reflex√£o simb√≥lica...")
            reflection_agent.reflect_on_tasks([code_agent])
            
            # Verificar se log foi criado
            log_path = Path("reflection/analysis_history.md")
            if log_path.exists():
                # Ler √∫ltimas linhas
                with open(log_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if lines:
                        last_lines = ''.join(lines[-5:])
                        print("‚úÖ Reflex√£o registrada!")
                        print(f"   √öltimas entradas: {last_lines[:100]}...")
                    else:
                        print("‚ö†Ô∏è Log vazio")
            else:
                print("‚ö†Ô∏è Log de reflex√£o n√£o encontrado")
            
            reflection_agent.close()
            print("‚úÖ Sistema de reflex√£o funcionando!")
            return True
            
        except ImportError as e:
            print(f"‚ùå Erro de import na reflex√£o: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Erro na reflex√£o: {e}")
            return False
    
    def test_memory_system(self):
        """Testa sistema de mem√≥ria."""
        print("\nüíæ Testando sistema de mem√≥ria...")
        
        try:
            # Testar paths
            from config.paths import IDENTITY_STATE, MEMORY_LOG, ensure_directories
            
            print("üìÅ Verificando diret√≥rios...")
            ensure_directories()
            print("‚úÖ Diret√≥rios criados!")
            
            # Testar arquivos de estado
            identity_exists = Path(IDENTITY_STATE).exists()
            memory_exists = Path(MEMORY_LOG).exists()
            
            print(f"üìÑ Identity state: {'Existe' if identity_exists else 'N√£o existe'}")
            print(f"üìÑ Memory log: {'Existe' if memory_exists else 'N√£o existe'}")
            
            # Testar mem√≥ria simb√≥lica
            try:
                from memory.symbolic.symbolic_memory import SymbolicMemory
                memory = SymbolicMemory()
                
                # Teste b√°sico
                test_data = {"TestAgent": {"pattern": "test", "quality": "high"}}
                memory.update_memory(test_data)
                
                print("‚úÖ Mem√≥ria simb√≥lica funcionando!")
                return True
                
            except ImportError:
                print("‚ö†Ô∏è Mem√≥ria simb√≥lica n√£o dispon√≠vel (usando fallback)")
                return True
                
        except Exception as e:
            print(f"‚ùå Erro no sistema de mem√≥ria: {e}")
            return False
    
    def test_llm_manager(self):
        """Testa o LLM Manager."""
        print("\nüß† Testando LLM Manager...")
        
        try:
            from core.llm.llm_manager import LLMManager
            
            # Instanciar LLMManager para usar as configura√ß√µes de settings.py
            llm_manager = LLMManager()
            
            # Testar informa√ß√µes dos modelos
            model_info = llm_manager.get_model_info()
            print(f"üìä Modelos configurados: {model_info.get('configured_models', {})}")
            print(f"üîó Ollama dispon√≠vel: {model_info.get('ollama_available', False)}")
            
            # Testar gera√ß√£o se Ollama estiver dispon√≠vel
            if model_info.get('ollama_available', False):
                print("üî• Testando gera√ß√£o via LLM Manager...")
                # Usar o modelo de c√≥digo configurado
                response = llm_manager.generate_code("def hello_world():")
                
                if response.success:
                    print("‚úÖ LLM Manager funcionando!")
                    print(f"   Modelo usado: {response.model}")
                    print(f"   Tokens: {response.tokens_used}")
                    print(f"   Tempo: {response.generation_time:.2f}s")
                    return True
                else:
                    print(f"‚ùå Falha na gera√ß√£o: {response.error}")
                    return False
            else:
                print("‚ö†Ô∏è Ollama n√£o dispon√≠vel, usando Mock")
                return True
                
        except ImportError as e:
            print(f"‚ùå Erro de import LLM Manager: {e}")
            return False
        except Exception as e:
            print(f"‚ùå Erro no LLM Manager: {e}")
            return False
    
    def test_dashboard_files(self):
        """Testa se arquivos do dashboard existem."""
        print("\nüìä Verificando arquivos do dashboard...")
        
        dashboard_files = [
            "interface/dashboard/streamlit_app.py",
            "config/paths.py", 
            "config/settings.py"
        ]
        
        all_exist = True
        for file_path in dashboard_files:
            path = Path(file_path)
            exists = path.exists()
            print(f"üìÑ {file_path}: {'‚úÖ' if exists else '‚ùå'}")
            
            if not exists:
                all_exist = False
        
        if all_exist:
            print("‚úÖ Todos os arquivos do dashboard est√£o presentes!")
            return True
        else:
            print("‚ö†Ô∏è Alguns arquivos est√£o faltando")
            return False
    
    def run_all_tests(self):
        """Executa todos os testes."""
        print("üöÄ INICIANDO TESTE COMPLETO DO SISTEMA RSCA")
        print("=" * 60)
        
        tests = [
            ("Conex√£o Ollama", self.test_ollama_connection),
            ("LLM Manager", self.test_llm_manager),
            ("Sistema de Mem√≥ria", self.test_memory_system),
            ("CodeAgent", self.test_code_agent),
            ("Pipeline Completo", self.test_pipeline),
            ("Sistema de Reflex√£o", self.test_reflection_system),
            ("Arquivos Dashboard", self.test_dashboard_files)
        ]
        
        results = {}
        ollama_models = []
        
        for test_name, test_func in tests:
            print(f"\n{'='*20} {test_name} {'='*20}")
            
            try:
                if test_name == "Conex√£o Ollama":
                    success, models = test_func()
                    ollama_models = models
                    results[test_name] = success
                elif test_name == "Gera√ß√£o Ollama" and ollama_models:
                    # Este teste agora usa um modelo espec√≠fico
                    results[test_name] = self.test_ollama_generation(self.ollama_code_model)
                else:
                    results[test_name] = test_func()
                    
            except Exception as e:
                print(f"‚ùå ERRO CR√çTICO em {test_name}: {e}")
                results[test_name] = False
        
        # Adicionar teste de gera√ß√£o se Ollama estiver funcionando
        if results.get("Conex√£o Ollama", False) and ollama_models:
            print(f"\n{'='*20} Gera√ß√£o Ollama {'='*20}")
            results["Gera√ß√£o Ollama"] = self.test_ollama_generation(self.ollama_code_model)
        
        # Relat√≥rio final
        self.print_final_report(results)
        
        return results
    
    def print_final_report(self, results):
        """Imprime relat√≥rio final."""
        execution_time = time.time() - self.start_time
        
        print("\n" + "="*60)
        print("üìã RELAT√ìRIO FINAL DE TESTES")
        print("="*60)
        
        passed = sum(1 for v in results.values() if v)
        total = len(results)
        success_rate = (passed / total) * 100 if total > 0 else 0
        
        print(f"‚è±Ô∏è  Tempo de execu√ß√£o: {execution_time:.2f}s")
        print(f"üìä Testes passaram: {passed}/{total} ({success_rate:.1f}%)")
        print()
        
        # Detalhamento por teste
        for test_name, success in results.items():
            status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
            print(f"{status:<12} {test_name}")
        
        print()
        
        # Diagn√≥stico e recomenda√ß√µes
        if success_rate >= 80:
            print("üéâ SISTEMA FUNCIONANDO BEM!")
            print("üí° Pronto para uso em desenvolvimento")
        elif success_rate >= 60:
            print("‚ö†Ô∏è  SISTEMA PARCIALMENTE FUNCIONAL")
            print("üí° Alguns componentes precisam de aten√ß√£o")
        else:
            print("üö® SISTEMA COM PROBLEMAS CR√çTICOS")
            print("üí° Revisar configura√ß√£o e depend√™ncias")
        
        # Recomenda√ß√µes espec√≠ficas
        if not results.get("Conex√£o Ollama", False):
            print("üîß Para usar LLMs reais: docker run -d -p 11434:11434 ollama/ollama")
        
        if not results.get("Sistema de Mem√≥ria", False):
            print("üîß Verificar config/paths.py e estrutura de diret√≥rios")
        
        if not results.get("CodeAgent", False):
            print("üîß Verificar imports e depend√™ncias dos agentes")
        
        print("\n" + "="*60)
        
        # Salvar relat√≥rio
        self.save_test_report(results, execution_time)
    
    def save_test_report(self, results, execution_time):
        """Salva relat√≥rio em arquivo."""
        try:
            report_dir = Path("logs")
            report_dir.mkdir(exist_ok=True)
            
            report_file = report_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"# Relat√≥rio de Testes RSCA - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**Tempo de execu√ß√£o:** {execution_time:.2f}s\n")
                f.write(f"**Testes passaram:** {sum(1 for v in results.values() if v)}/{len(results)}\n\n")
                
                f.write("## Resultados por Teste\n\n")
                for test_name, success in results.items():
                    status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
                    f.write(f"- **{test_name}:** {status}\n")
                
                f.write(f"\n## Sistema\n")
                f.write(f"- Python: {sys.version}\n")
                f.write(f"- Diret√≥rio: {Path.cwd()}\n")
            
            print(f"üìÑ Relat√≥rio salvo em: {report_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è N√£o foi poss√≠vel salvar relat√≥rio: {e}")


def main():
    """Fun√ß√£o principal."""
    print(f"üî¨ Teste do Sistema RSCA")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üêç Python {sys.version.split()[0]}")
    print(f"üìÅ Diret√≥rio: {Path.cwd()}")
    
    tester = RSCASystemTest()
    results = tester.run_all_tests()
    
    # C√≥digo de sa√≠da baseado no sucesso
    success_rate = sum(1 for v in results.values() if v) / len(results) if results else 0
    exit_code = 0 if success_rate >= 0.8 else 1
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
