#!/usr/bin/env python3
"""
Teste Ãºnico e completo do sistema RSCA
Substitui todos os outros arquivos de teste
"""

import sys
import time
import requests
from pathlib import Path
from datetime import datetime

# Adicionar path do projeto
sys.path.append(str(Path(__file__).parent.parent.parent))

class RSCASystemTest:
    def __init__(self):
        self.results = {}
        self.start_time = time.time()
        # Definir modelos a serem usados nos testes
        self.ollama_code_model = "deepseek-r1:1.5b"
        self.ollama_general_model = "qwen2:1.5b"
    
    def test_ollama_connection(self):
        """Testa conexÃ£o com Ollama e verifica se os modelos necessÃ¡rios estÃ£o presentes."""
        print("ğŸ”— Testando conexÃ£o Ollama...")
        
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json().get("models", [])
                model_names = [m["name"] for m in models_data]
                
                print(f"âœ… Ollama funcionando! Modelos disponÃ­veis: {len(model_names)}")
                for model in model_names:
                    print(f"   â€¢ {model}")
                
                # Verificar se os modelos especÃ­ficos estÃ£o presentes
                has_code_model = any(m.startswith(self.ollama_code_model.split(':')[0]) for m in model_names)
                has_general_model = any(m.startswith(self.ollama_general_model.split(':')[0]) for m in model_names)

                if has_code_model and has_general_model:
                    print(f"âœ… Modelos '{self.ollama_code_model}' e '{self.ollama_general_model}' encontrados.")
                    return True, model_names
                else:
                    print(f"âŒ Modelos '{self.ollama_code_model}' ou '{self.ollama_general_model}' nÃ£o encontrados.")
                    return False, model_names
                
            else:
                print(f"âŒ Ollama retornou status {response.status_code}")
                return False, []
                
        except Exception as e:
            print(f"âŒ Erro na conexÃ£o: {e}")
            return False, []
    
    def test_ollama_generation(self, model_to_use):
        """Testa geraÃ§Ã£o com Ollama usando um modelo especÃ­fico."""
        print(f"\nğŸ¤– Testando geraÃ§Ã£o de cÃ³digo com modelo: {model_to_use}...")
        
        try:
            payload = {
                "model": model_to_use,
                "prompt": "def calculate_sum(a, b): return",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 100
                }
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "").strip()
                
                if content:
                    print("âœ… GeraÃ§Ã£o funcionando!")
                    print(f"ğŸ“„ CÃ³digo gerado:")
                    print("-" * 30)
                    print(content[:150] + "..." if len(content) > 150 else content)
                    print("-" * 30)
                    return True
                else:
                    print("âš ï¸ Resposta vazia")
                    return False
            else:
                print(f"âŒ Erro HTTP {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"âŒ Erro na geraÃ§Ã£o: {e}")
            return False
    
    def test_code_agent(self):
        """Testa CodeAgent."""
        print("\nğŸ› ï¸ Testando CodeAgent...")
        
        try:
            from core.agents.code_agent import CodeAgent
            
            # Criar agente, garantindo que use o modelo correto
            # O CodeAgent deve pegar o modelo de config.settings.py
            agent = CodeAgent(use_mock=False)
            
            # Tarefa simples
            task = "Criar funÃ§Ã£o que multiplica dois nÃºmeros"
            print(f"ğŸ“ Tarefa: {task}")
            
            result = agent.execute_task(task)
            
            print(f"ğŸ“Š Resultado:")
            print(f"   Sucesso: {result.success}")
            print(f"   Qualidade: {result.quality_score:.1f}/10")
            print(f"   Modelo: {getattr(result, 'llm_used', 'unknown')}")
            
            if result.success:
                print(f"âœ… CodeAgent funcionando!")
                if hasattr(result, 'execution_result') and result.execution_result:
                    print(f"   ExecuÃ§Ã£o: {result.execution_result.strip()}")
                return True
            else:
                print(f"âŒ CodeAgent falhou: {getattr(result, 'error', 'Erro desconhecido')}")
                return False
                
        except ImportError as e:
            print(f"âŒ Erro de import: {e}")
            print("ğŸ’¡ Execute: python scripts/fixes/fix_imports.py")
            return False
        except Exception as e:
            print(f"âŒ Erro no CodeAgent: {e}")
            return False
    
    def test_pipeline(self):
        """Testa pipeline completo."""
        print("\nğŸ”„ Testando pipeline Code->Test->Doc...")
        
        try:
            from core.agents.code_agent import CodeAgent
            from core.agents.test_agent import TestAgent
            from core.agents.doc_agent import DocumentationAgent
            
            # Criar agentes
            code_agent = CodeAgent(use_mock=False)
            test_agent = TestAgent()
            doc_agent = DocumentationAgent()
            
            task = "FunÃ§Ã£o que calcula Ã¡rea de um cÃ­rculo"
            print(f"ğŸ“ Tarefa: {task}")
            
            # 1. CÃ³digo
            print("1ï¸âƒ£ Gerando cÃ³digo...")
            code_result = code_agent.execute_task(task)
            
            if not code_result.success:
                print("âŒ Falha na geraÃ§Ã£o de cÃ³digo")
                return False
            
            # 2. Testes
            print("2ï¸âƒ£ Gerando testes...")
            try:
                test_code = test_agent.generate_tests(code_result.code, "calcular_area_circulo")
                print("âœ… Testes gerados!")
                
                # Mostrar primeira linha do teste
                first_line = test_code.split('\n')[0] if test_code else "Vazio"
                print(f"   Primeira linha: {first_line}")
                
            except Exception as e:
                print(f"âš ï¸ Erro nos testes: {e}")
            
            # 3. DocumentaÃ§Ã£o
            print("3ï¸âƒ£ Gerando documentaÃ§Ã£o...")
            try:
                doc_agent.create_docs(code_result.code)
                print("âœ… DocumentaÃ§Ã£o gerada!")
                print(f"   ConteÃºdo: {doc_agent.latest_output[:100]}...")
                
            except Exception as e:
                print(f"âš ï¸ Erro na documentaÃ§Ã£o: {e}")
            
            print("âœ… Pipeline funcionando!")
            return True
            
        except ImportError as e:
            print(f"âŒ Erro de import no pipeline: {e}")
            return False
        except Exception as e:
            print(f"âŒ Erro no pipeline: {e}")
            return False
    
    def test_reflection_system(self):
        """Testa sistema de reflexÃ£o."""
        print("\nğŸ§  Testando sistema de reflexÃ£o...")
        
        try:
            from core.agents.reflection_agent import ReflectionAgent
            from core.agents.code_agent import CodeAgent
            
            # Criar agentes
            code_agent = CodeAgent(use_mock=True)  # Mock para rapidez
            reflection_agent = ReflectionAgent()
            
            # Gerar algum cÃ³digo
            code_agent.execute_task("FunÃ§Ã£o simples de teste")
            
            # Testar reflexÃ£o
            print("ğŸ” Executando reflexÃ£o simbÃ³lica...")
            reflection_agent.reflect_on_tasks([code_agent])
            
            # Verificar se log foi criado
            log_path = Path("reflection/analysis_history.md")
            if log_path.exists():
                # Ler Ãºltimas linhas
                with open(log_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if lines:
                        last_lines = ''.join(lines[-5:])
                        print("âœ… ReflexÃ£o registrada!")
                        print(f"   Ãšltimas entradas: {last_lines[:100]}...")
                    else:
                        print("âš ï¸ Log vazio")
            else:
                print("âš ï¸ Log de reflexÃ£o nÃ£o encontrado")
            
            reflection_agent.close()
            print("âœ… Sistema de reflexÃ£o funcionando!")
            return True
            
        except ImportError as e:
            print(f"âŒ Erro de import na reflexÃ£o: {e}")
            return False
        except Exception as e:
            print(f"âŒ Erro na reflexÃ£o: {e}")
            return False
    
    def test_memory_system(self):
        """Testa sistema de memÃ³ria."""
        print("\nğŸ’¾ Testando sistema de memÃ³ria...")
        
        try:
            # Testar paths
            from config.paths import IDENTITY_STATE, MEMORY_LOG, ensure_directories
            
            print("ğŸ“ Verificando diretÃ³rios...")
            ensure_directories()
            print("âœ… DiretÃ³rios criados!")
            
            # Testar arquivos de estado
            identity_exists = Path(IDENTITY_STATE).exists()
            memory_exists = Path(MEMORY_LOG).exists()
            
            print(f"ğŸ“„ Identity state: {'Existe' if identity_exists else 'NÃ£o existe'}")
            print(f"ğŸ“„ Memory log: {'Existe' if memory_exists else 'NÃ£o existe'}")
            
            # Testar memÃ³ria simbÃ³lica
            try:
                from memory.symbolic.symbolic_memory import SymbolicMemory
                memory = SymbolicMemory()
                
                # Teste bÃ¡sico
                test_data = {"TestAgent": {"pattern": "test", "quality": "high"}}
                memory.update_memory(test_data)
                
                print("âœ… MemÃ³ria simbÃ³lica funcionando!")
                return True
                
            except ImportError:
                print("âš ï¸ MemÃ³ria simbÃ³lica nÃ£o disponÃ­vel (usando fallback)")
                return True
                
        except Exception as e:
            print(f"âŒ Erro no sistema de memÃ³ria: {e}")
            return False
    
    def test_llm_manager(self):
        """Testa o LLM Manager."""
        print("\nğŸ§  Testando LLM Manager...")
        
        try:
            from core.llm.llm_manager import LLMManager
            
            # Instanciar LLMManager para usar as configuraÃ§Ãµes de settings.py
            llm_manager = LLMManager()
            
            # Testar informaÃ§Ãµes dos modelos
            model_info = llm_manager.get_model_info()
            print(f"ğŸ“Š Modelos configurados: {model_info.get('configured_models', {})}")
            print(f"ğŸ”— Ollama disponÃ­vel: {model_info.get('ollama_available', False)}")
            
            # Testar geraÃ§Ã£o se Ollama estiver disponÃ­vel
            if model_info.get('ollama_available', False):
                print("ğŸ”¥ Testando geraÃ§Ã£o via LLM Manager...")
                # Usar o modelo de cÃ³digo configurado
                response = llm_manager.generate_code("def hello_world():")
                
                if response.success:
                    print("âœ… LLM Manager funcionando!")
                    print(f"   Modelo usado: {response.model}")
                    print(f"   Tokens: {response.tokens_used}")
                    print(f"   Tempo: {response.generation_time:.2f}s")
                    return True
                else:
                    print(f"âŒ Falha na geraÃ§Ã£o: {response.error}")
                    return False
            else:
                print("âš ï¸ Ollama nÃ£o disponÃ­vel, usando Mock")
                return True
                
        except ImportError as e:
            print(f"âŒ Erro de import LLM Manager: {e}")
            return False
        except Exception as e:
            print(f"âŒ Erro no LLM Manager: {e}")
            return False
    
    def test_dashboard_files(self):
        """Testa se arquivos do dashboard existem."""
        print("\nğŸ“Š Verificando arquivos do dashboard...")
        
        dashboard_files = [
            "interface/dashboard/streamlit_app.py",
            "config/paths.py", 
            "config/settings.py"
        ]
        
        all_exist = True
        for file_path in dashboard_files:
            path = Path(file_path)
            exists = path.exists()
            print(f"ğŸ“„ {file_path}: {'âœ…' if exists else 'âŒ'}")
            
            if not exists:
                all_exist = False
        
        if all_exist:
            print("âœ… Todos os arquivos do dashboard estÃ£o presentes!")
            return True
        else:
            print("âš ï¸ Alguns arquivos estÃ£o faltando")
            return False
    
    def run_all_tests(self):
        """Executa todos os testes."""
        print("ğŸš€ INICIANDO TESTE COMPLETO DO SISTEMA RSCA")
        print("=" * 60)
        
        tests = [
            ("ConexÃ£o Ollama", self.test_ollama_connection),
            ("LLM Manager", self.test_llm_manager),
            ("Sistema de MemÃ³ria", self.test_memory_system),
            ("CodeAgent", self.test_code_agent),
            ("Pipeline Completo", self.test_pipeline),
            ("Sistema de ReflexÃ£o", self.test_reflection_system),
            ("Arquivos Dashboard", self.test_dashboard_files)
        ]
        
        results = {}
        ollama_models = []
        
        for test_name, test_func in tests:
            print(f"\n{'='*20} {test_name} {'='*20}")
            
            try:
                if test_name == "ConexÃ£o Ollama":
                    success, models = test_func()
                    ollama_models = models
                    results[test_name] = success
                elif test_name == "GeraÃ§Ã£o Ollama" and ollama_models:
                    # Este teste agora usa um modelo especÃ­fico
                    results[test_name] = self.test_ollama_generation(self.ollama_code_model)
                else:
                    results[test_name] = test_func()
                    
            except Exception as e:
                print(f"âŒ ERRO CRÃTICO em {test_name}: {e}")
                results[test_name] = False
        
        # Adicionar teste de geraÃ§Ã£o se Ollama estiver funcionando
        if results.get("ConexÃ£o Ollama", False) and ollama_models:
            print(f"\n{'='*20} GeraÃ§Ã£o Ollama {'='*20}")
            results["GeraÃ§Ã£o Ollama"] = self.test_ollama_generation(self.ollama_code_model)
        
        # RelatÃ³rio final
        self.print_final_report(results)
        
        return results
    
    def print_final_report(self, results):
        """Imprime relatÃ³rio final."""
        execution_time = time.time() - self.start_time
        
        print("\n" + "="*60)
        print("ğŸ“‹ RELATÃ“RIO FINAL DE TESTES")
        print("="*60)
        
        passed = sum(1 for v in results.values() if v)
        total = len(results)
        success_rate = (passed / total) * 100 if total > 0 else 0
        
        print(f"â±ï¸  Tempo de execuÃ§Ã£o: {execution_time:.2f}s")
        print(f"ğŸ“Š Testes passaram: {passed}/{total} ({success_rate:.1f}%)")
        print()
        
        # Detalhamento por teste
        for test_name, success in results.items():
            status = "âœ… PASSOU" if success else "âŒ FALHOU"
            print(f"{status:<12} {test_name}")
        
        print()
        
        # DiagnÃ³stico e recomendaÃ§Ãµes
        if success_rate >= 80:
            print("ğŸ‰ SISTEMA FUNCIONANDO BEM!")
            print("ğŸ’¡ Pronto para uso em desenvolvimento")
        elif success_rate >= 60:
            print("âš ï¸  SISTEMA PARCIALMENTE FUNCIONAL")
            print("ğŸ’¡ Alguns componentes precisam de atenÃ§Ã£o")
        else:
            print("ğŸš¨ SISTEMA COM PROBLEMAS CRÃTICOS")
            print("ğŸ’¡ Revisar configuraÃ§Ã£o e dependÃªncias")
        
        # RecomendaÃ§Ãµes especÃ­ficas
        if not results.get("ConexÃ£o Ollama", False):
            print("ğŸ”§ Para usar LLMs reais: docker run -d -p 11434:11434 ollama/ollama")
        
        if not results.get("Sistema de MemÃ³ria", False):
            print("ğŸ”§ Verificar config/paths.py e estrutura de diretÃ³rios")
        
        if not results.get("CodeAgent", False):
            print("ğŸ”§ Verificar imports e dependÃªncias dos agentes")
        
        print("\n" + "="*60)
        
        # Salvar relatÃ³rio
        self.save_test_report(results, execution_time)
    
    def save_test_report(self, results, execution_time):
        """Salva relatÃ³rio em arquivo."""
        try:
            report_dir = Path("logs")
            report_dir.mkdir(exist_ok=True)
            
            report_file = report_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"# RelatÃ³rio de Testes RSCA - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"**Tempo de execuÃ§Ã£o:** {execution_time:.2f}s\n")
                f.write(f"**Testes passaram:** {sum(1 for v in results.values() if v)}/{len(results)}\n\n")
                
                f.write("## Resultados por Teste\n\n")
                for test_name, success in results.items():
                    status = "âœ… PASSOU" if success else "âŒ FALHOU"
                    f.write(f"- **{test_name}:** {status}\n")
                
                f.write(f"\n## Sistema\n")
                f.write(f"- Python: {sys.version}\n")
                f.write(f"- DiretÃ³rio: {Path.cwd()}\n")
            
            print(f"ğŸ“„ RelatÃ³rio salvo em: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸ NÃ£o foi possÃ­vel salvar relatÃ³rio: {e}")


def main():
    """FunÃ§Ã£o principal."""
    print(f"ğŸ”¬ Teste do Sistema RSCA")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python {sys.version.split()[0]}")
    print(f"ğŸ“ DiretÃ³rio: {Path.cwd()}")
    
    tester = RSCASystemTest()
    results = tester.run_all_tests()
    
    # CÃ³digo de saÃ­da baseado no sucesso
    success_rate = sum(1 for v in results.values() if v) / len(results) if results else 0
    exit_code = 0 if success_rate >= 0.8 else 1
    
    sys.exit(exit_code)


if __name__ == "__main__":
    main()
