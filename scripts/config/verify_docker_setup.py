#!/usr/bin/env python3
"""
Script para verificar se o setup Docker + Ollama + Modelos estÃ¡ funcionando
"""

import requests
import json
import time
import subprocess
from datetime import datetime

class DockerSetupVerifier:
    def __init__(self):
        self.ollama_host = "http://localhost:11434"
        self.neo4j_host = "http://localhost:7474"
        self.chromadb_host = "http://localhost:8000"
        
    def check_containers(self):
        """Verifica se containers estÃ£o rodando"""
        print("ğŸ³ Verificando containers Docker...")
        
        try:
            result = subprocess.run(['docker', 'ps'], capture_output=True, text=True)
            containers = result.stdout
            
            expected_containers = [
                "rsca-ollama",
                "rsca-neo4j", 
                "rsca-chromadb"
            ]
            
            status = {}
            for container in expected_containers:
                if container in containers:
                    print(f"âœ… {container} - Rodando")
                    status[container] = True
                else:
                    print(f"âŒ {container} - NÃ£o encontrado")
                    status[container] = False
            
            # Verificar container de setup
            if "rsca-ollama-setup" in containers:
                print("ğŸ”„ rsca-ollama-setup - Ainda instalando modelos...")
            elif "rsca-ollama-setup" in result.stdout:
                print("âœ… rsca-ollama-setup - InstalaÃ§Ã£o de modelos concluÃ­da")
            
            return status
            
        except Exception as e:
            print(f"âŒ Erro ao verificar containers: {e}")
            return {}
    
    def check_ollama_api(self):
        """Verifica se API do Ollama estÃ¡ respondendo"""
        print(f"\nğŸ¤– Verificando Ollama API ({self.ollama_host})...")
        
        try:
            # Verificar se Ollama estÃ¡ respondendo
            response = requests.get(f"{self.ollama_host}/api/tags", timeout=10)
            
            if response.status_code == 200:
                print("âœ… Ollama API estÃ¡ respondendo!")
                
                # Listar modelos instalados
                models_data = response.json()
                models = [model["name"] for model in models_data.get("models", [])]
                
                print(f"ğŸ“¦ Modelos instalados: {len(models)}")
                for model in models:
                    print(f"   â€¢ {model}")
                
                # Verificar modelos esperados
                expected_models = ["llama3:8b", "codellama:7b", "codellama:13b"]
                missing_models = [m for m in expected_models if m not in models]
                
                if missing_models:
                    print(f"âš ï¸ Modelos faltando: {missing_models}")
                    print("ğŸ’¡ Aguarde a instalaÃ§Ã£o automÃ¡tica ou execute:")
                    for model in missing_models:
                        print(f"   docker exec rsca-ollama ollama pull {model}")
                else:
                    print("âœ… Todos os modelos esperados estÃ£o instalados!")
                
                return True, models
            else:
                print(f"âŒ Ollama API retornou status: {response.status_code}")
                return False, []
                
        except requests.exceptions.ConnectionError:
            print("âŒ NÃ£o foi possÃ­vel conectar ao Ollama")
            print("ğŸ’¡ Verifique se o container rsca-ollama estÃ¡ rodando")
            return False, []
        except Exception as e:
            print(f"âŒ Erro ao verificar Ollama: {e}")
            return False, []
    
    def test_model_generation(self, model="llama3:8b"):
        """Testa geraÃ§Ã£o de texto com um modelo"""
        print(f"\nğŸ§ª Testando geraÃ§Ã£o com {model}...")
        
        try:
            payload = {
                "model": model,
                "prompt": "Responda apenas: 'Funcionando!'",
                "stream": False
            }
            
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                generated_text = data.get("response", "").strip()
                
                print(f"âœ… {model} respondeu: '{generated_text}'")
                return True, generated_text
            else:
                print(f"âŒ Erro na geraÃ§Ã£o: {response.status_code}")
                return False, ""
                
        except Exception as e:
            print(f"âŒ Erro ao testar modelo: {e}")
            return False, ""
    
    def check_neo4j(self):
        """Verifica conexÃ£o com Neo4j"""
        print(f"\nğŸ—ƒï¸ Verificando Neo4j ({self.neo4j_host})...")
        
        try:
            response = requests.get(self.neo4j_host, timeout=10)
            if response.status_code == 200:
                print("âœ… Neo4j estÃ¡ acessÃ­vel!")
                return True
            else:
                print(f"âš ï¸ Neo4j retornou status: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ Erro ao conectar Neo4j: {e}")
            return False
    
    def check_chromadb(self):
        """Verifica conexÃ£o com ChromaDB"""
        print(f"\nğŸ” Verificando ChromaDB ({self.chromadb_host})...")
        
        try:
            response = requests.get(f"{self.chromadb_host}/api/v1/heartbeat", timeout=10)
            if response.status_code == 200:
                print("âœ… ChromaDB estÃ¡ acessÃ­vel!")
                return True
            else:
                print(f"âš ï¸ ChromaDB retornou status: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ Erro ao conectar ChromaDB: {e}")
            return False
    
    def test_integration(self):
        """Testa integraÃ§Ã£o completa"""
        print(f"\nğŸ”— Testando integraÃ§Ã£o completa...")
        
        try:
            # Importar e testar LLM client
            import sys
            from pathlib import Path
            sys.path.append(str(Path(__file__).parent.parent))
            
            from core.llm.ollama_client import OllamaClient
            
            client = OllamaClient()
            
            if client.is_available():
                print("âœ… Cliente Ollama conectado!")
                
                # Testar geraÃ§Ã£o
                response = client.generate("llama3:8b", "def hello():")
                if response.success:
                    print("âœ… GeraÃ§Ã£o via cliente funcionando!")
                    print(f"ğŸ“ Resposta: {response.content[:100]}...")
                else:
                    print(f"âŒ Erro na geraÃ§Ã£o: {response.error}")
                
                return True
            else:
                print("âŒ Cliente Ollama nÃ£o conseguiu conectar")
                return False
                
        except ImportError as e:
            print(f"âš ï¸ MÃ³dulo nÃ£o encontrado: {e}")
            print("ğŸ’¡ Implemente primeiro: core/llm/ollama_client.py")
            return False
        except Exception as e:
            print(f"âŒ Erro na integraÃ§Ã£o: {e}")
            return False
    
    def generate_report(self):
        """Gera relatÃ³rio completo do setup"""
        print("\n" + "="*60)
        print("ğŸ“‹ RELATÃ“RIO COMPLETO DO SETUP")
        print("="*60)
        
        # Verificar tudo
        containers_status = self.check_containers()
        ollama_ok, models = self.check_ollama_api()
        neo4j_ok = self.check_neo4j()
        chromadb_ok = self.check_chromadb()
        
        # Testar modelos se disponÃ­veis
        model_tests = {}
        if ollama_ok and models:
            for model in ["llama3:8b", "codellama:7b"]:
                if model in models:
                    success, response = self.test_model_generation(model)
                    model_tests[model] = success
        
        # Testar integraÃ§Ã£o
        integration_ok = self.test_integration()
        
        # Resumo final
        print(f"\nğŸ¯ RESUMO FINAL")
        print("-" * 30)
        
        total_checks = 0
        passed_checks = 0
        
        # Containers
        for container, status in containers_status.items():
            total_checks += 1
            if status:
                passed_checks += 1
        
        # ServiÃ§os
        services = [
            ("Ollama API", ollama_ok),
            ("Neo4j", neo4j_ok), 
            ("ChromaDB", chromadb_ok),
            ("IntegraÃ§Ã£o", integration_ok)
        ]
        
        for service, status in services:
            total_checks += 1
            if status:
                passed_checks += 1
            print(f"{'âœ…' if status else 'âŒ'} {service}")
        
        # Modelos
        if model_tests:
            for model, status in model_tests.items():
                total_checks += 1
                if status:
                    passed_checks += 1
                print(f"{'âœ…' if status else 'âŒ'} {model}")
        
        # Score final
        score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
        print(f"\nğŸ“Š Score: {passed_checks}/{total_checks} ({score:.1f}%)")
        
        if score >= 80:
            print("ğŸ‰ Setup estÃ¡ funcionando bem!")
            print("ğŸš€ PrÃ³ximo passo: python scripts/test_llm_integration.py")
        elif score >= 60:
            print("âš ï¸ Setup parcialmente funcional")
            print("ğŸ”§ Verifique os itens com âŒ acima")
        else:
            print("âŒ Setup precisa de correÃ§Ãµes")
            print("ğŸ†˜ Verifique logs do Docker: docker-compose logs")
        
        return score

def main():
    print("ğŸ” VERIFICAÃ‡ÃƒO DO SETUP DOCKER + OLLAMA")
    print("="*50)
    print(f"ğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    verifier = DockerSetupVerifier()
    score = verifier.generate_report()
    
    print(f"\nğŸ“‹ VerificaÃ§Ã£o concluÃ­da com score: {score:.1f}%")

if __name__ == "__main__":
    main()