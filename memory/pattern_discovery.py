# memory/pattern_discovery.py
"""
Sistema de Descoberta de PadrÃµes - Identifica padrÃµes emergentes nas experiÃªncias
Integra com sistema simbÃ³lico atual para manter compatibilidade

CORREÃ‡Ã•ES APLICADAS:
- Garantir criaÃ§Ã£o do campo 'symbolic_traits' 
- Mapeamento robusto de padrÃµes para traits simbÃ³licos
- IntegraÃ§Ã£o simbÃ³lica mais completa
"""

import yaml
import json
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import Counter, defaultdict
from sklearn.cluster import DBSCAN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from memory.hybrid_store import HybridMemoryStore
from config.paths import SYMBOLIC_TIMELINE, MEMORY_LOG, IDENTITY_STATE

@dataclass
class DiscoveredPattern:
    """PadrÃ£o descoberto pelo sistema"""
    id: str
    name: str
    description: str
    template: str
    success_rate: float
    usage_count: int
    contexts: List[str]
    quality_impact: float
    discovery_date: datetime
    related_experiences: List[str]
    confidence_score: float


class PatternDiscoveryEngine:
    """
    Engine para descoberta automÃ¡tica de padrÃµes de codificaÃ§Ã£o
    """
    
    def __init__(self, memory_store: HybridMemoryStore):
        self.memory = memory_store
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 3)
        )
        self.discovered_patterns = []
        self.pattern_evolution = {}
        
    def discover_patterns(self, min_occurrences: int = 3, 
                         min_success_rate: float = 0.7) -> List[DiscoveredPattern]:
        """
        Descobre padrÃµes emergentes nas experiÃªncias armazenadas
        """
        print("ğŸ” Iniciando descoberta de padrÃµes...")
        
        # 1. Coletar experiÃªncias recentes
        experiences = self._collect_recent_experiences()
        if len(experiences) < min_occurrences:
            print(f"âš ï¸ Poucas experiÃªncias ({len(experiences)}) para descobrir padrÃµes")
            return []
        
        # 2. AnÃ¡lise de clustering por similaridade
        code_clusters = self._cluster_by_code_similarity(experiences)
        
        # 3. AnÃ¡lise de padrÃµes por domÃ­nio/tarefa  
        task_patterns = self._analyze_task_patterns(experiences)
        
        # 4. AnÃ¡lise de qualidade vs abordagem
        quality_patterns = self._analyze_quality_patterns(experiences)
        
        # 5. Combinar e validar padrÃµes
        all_patterns = code_clusters + task_patterns + quality_patterns
        validated_patterns = self._validate_patterns(all_patterns, min_occurrences, min_success_rate)
        
        # 6. Atualizar padrÃµes conhecidos
        self._update_pattern_evolution(validated_patterns)
        
        # 7. Integrar com sistema simbÃ³lico atual - CORRIGIDO
        self._integrate_with_symbolic_system(validated_patterns)
        
        self.discovered_patterns = validated_patterns
        print(f"âœ… {len(validated_patterns)} padrÃµes descobertos")
        
        return validated_patterns
    
    def _collect_recent_experiences(self, days_ago: int = 30) -> List[Dict]:
        """Coleta experiÃªncias recentes do GraphRAG e sistema YAML"""
        experiences = []
        
        try:
            # ExperiÃªncias do GraphRAG
            if self.memory.enable_graphrag:
                with self.memory.neo4j.session() as session:
                    result = session.run("""
                        MATCH (e:Experience)-[:GENERATED_CODE]->(c:Code)
                        WHERE e.timestamp > datetime() - duration('P30D')
                        RETURN e, c
                        ORDER BY e.timestamp DESC
                        LIMIT 100
                    """)
                    
                    for record in result:
                        exp = record['e']
                        code = record['c']
                        experiences.append({
                            'id': exp['id'],
                            'task': exp['task_description'],
                            'code': code['content'],
                            'quality': exp['quality_score'], 
                            'success': exp['execution_success'],
                            'agent': exp['agent_name'],
                            'timestamp': exp['timestamp'],
                            'source': 'graphrag'
                        })
            
            # ExperiÃªncias do sistema YAML atual (compatibilidade)
            yaml_experiences = self._extract_yaml_experiences()
            experiences.extend(yaml_experiences)
            
        except Exception as e:
            print(f"âš ï¸ Erro ao coletar experiÃªncias: {e}")
            # Fallback para sistema YAML apenas
            experiences = self._extract_yaml_experiences()
        
        return experiences
    
    def _extract_yaml_experiences(self) -> List[Dict]:
        """Extrai experiÃªncias do sistema YAML atual para compatibilidade"""
        experiences = []
        
        try:
            # Carregar dados YAML atuais
            with open(MEMORY_LOG, 'r', encoding='utf-8') as f:
                memory_data = yaml.safe_load(f) or {}
            
            with open(SYMBOLIC_TIMELINE, 'r', encoding='utf-8') as f:
                timeline_data = yaml.safe_load(f) or {}
            
            # Converter dados YAML em formato compatÃ­vel
            timeline_entries = timeline_data.get('linha_temporal', [])
            
            for entry in timeline_entries[-20:]:  # Ãšltimas 20 entradas
                # Simular experiÃªncia baseada nos dados simbÃ³licos
                experiences.append({
                    'id': f"yaml_{entry.get('ciclo', 0)}",
                    'task': entry.get('evento', 'Tarefa desconhecida'),
                    'code': f"# CÃ³digo simbÃ³lico do ciclo {entry.get('ciclo', 0)}",
                    'quality': 7.0,  # Assumir qualidade mÃ©dia
                    'success': True,
                    'agent': 'CodeAgent',
                    'timestamp': entry.get('timestamp', datetime.now().isoformat()),
                    'source': 'yaml_symbolic'
                })
                
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair experiÃªncias YAML: {e}")
        
        return experiences
    
    def _cluster_by_code_similarity(self, experiences: List[Dict]) -> List[DiscoveredPattern]:
        """Agrupa experiÃªncias por similaridade de cÃ³digo"""
        patterns = []
        
        try:
            # Extrair cÃ³digos vÃ¡lidos
            codes = [exp['code'] for exp in experiences if exp.get('code') and len(exp['code'].strip()) > 20]
            
            if len(codes) < 3:
                return patterns
            
            # Vetorizar cÃ³digos
            code_vectors = self.vectorizer.fit_transform(codes)
            
            # Clustering DBSCAN
            clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine')
            clusters = clustering.fit_predict(code_vectors.toarray())
            
            # Processar cada cluster
            cluster_groups = defaultdict(list)
            for i, cluster_id in enumerate(clusters):
                if cluster_id != -1:  # Ignorar outliers
                    cluster_groups[cluster_id].append(i)
            
            for cluster_id, indices in cluster_groups.items():
                if len(indices) >= 2:  # MÃ­nimo 2 experiÃªncias similares
                    cluster_experiences = [experiences[codes.index(experiences[i]['code'])] for i in indices]
                    pattern = self._extract_code_pattern(cluster_experiences, f"code_cluster_{cluster_id}")
                    if pattern:
                        patterns.append(pattern)
                        
        except Exception as e:
            print(f"âš ï¸ Erro no clustering de cÃ³digo: {e}")
        
        return patterns
    
    def _extract_code_pattern(self, similar_experiences: List[Dict], pattern_id: str) -> Optional[DiscoveredPattern]:
        """Extrai padrÃ£o comum de experiÃªncias similares"""
        try:
            # Calcular mÃ©tricas do padrÃ£o
            success_rate = sum(1 for exp in similar_experiences if exp.get('success', False)) / len(similar_experiences)
            avg_quality = sum(exp.get('quality', 0) for exp in similar_experiences) / len(similar_experiences)
            
            # Extrair caracterÃ­sticas comuns
            codes = [exp.get('code', '') for exp in similar_experiences]
            tasks = [exp.get('task', '') for exp in similar_experiences]
            
            # Identificar template comum
            template = self._find_common_code_structure(codes)
            
            # Identificar contextos
            contexts = list(set([self._extract_context(task) for task in tasks]))
            
            # Gerar descriÃ§Ã£o
            description = self._generate_pattern_description(template, contexts, similar_experiences)
            
            return DiscoveredPattern(
                id=pattern_id,
                name=f"PadrÃ£o de {contexts[0] if contexts else 'CÃ³digo'}",
                description=description,
                template=template,
                success_rate=success_rate,
                usage_count=len(similar_experiences),
                contexts=contexts,
                quality_impact=avg_quality,
                discovery_date=datetime.now(),
                related_experiences=[exp.get('id', '') for exp in similar_experiences],
                confidence_score=min(success_rate + (len(similar_experiences) / 10), 1.0)
            )
            
        except Exception as e:
            print(f"âš ï¸ Erro ao extrair padrÃ£o: {e}")
            return None
    
    def _find_common_code_structure(self, codes: List[str]) -> str:
        """Encontra estrutura comum entre cÃ³digos"""
        if not codes:
            return ""
        
        # AnÃ¡lise simples de estruturas comuns
        common_patterns = []
        
        # Verificar padrÃµes de funÃ§Ã£o
        if all('def ' in code for code in codes):
            common_patterns.append("def {function_name}({parameters}):")
        
        # Verificar padrÃµes de validaÃ§Ã£o
        if any(pattern in ' '.join(codes).lower() for pattern in ['if not', 'raise', 'assert']):
            common_patterns.append("    # ValidaÃ§Ã£o de entrada")
        
        # Verificar padrÃµes de retorno
        if all('return' in code for code in codes):
            common_patterns.append("    return {result}")
        
        # Verificar padrÃµes de erro
        if any('try:' in code for code in codes):
            common_patterns.append("    # Tratamento de erro")
        
        template = '\n'.join(common_patterns) if common_patterns else codes[0][:100] + "..."
        return template
    
    def _analyze_task_patterns(self, experiences: List[Dict]) -> List[DiscoveredPattern]:
        """Analisa padrÃµes por tipo de tarefa"""
        patterns = []
        
        try:
            # Agrupar por domÃ­nio de tarefa
            task_groups = defaultdict(list)
            
            for exp in experiences:
                domain = self._extract_context(exp.get('task', ''))
                task_groups[domain].append(exp)
            
            # Analisar cada grupo
            for domain, group_experiences in task_groups.items():
                if len(group_experiences) >= 3:  # MÃ­nimo 3 experiÃªncias por domÃ­nio
                    pattern = self._create_task_pattern(domain, group_experiences)
                    if pattern:
                        patterns.append(pattern)
                        
        except Exception as e:
            print(f"âš ï¸ Erro na anÃ¡lise de padrÃµes de tarefa: {e}")
        
        return patterns
    
    def _create_task_pattern(self, domain: str, experiences: List[Dict]) -> Optional[DiscoveredPattern]:
        """Cria padrÃ£o baseado em domÃ­nio de tarefa"""
        try:
            # Calcular mÃ©tricas
            success_rate = sum(1 for exp in experiences if exp.get('success', False)) / len(experiences)
            avg_quality = sum(exp.get('quality', 0) for exp in experiences) / len(experiences)
            
            # Identificar abordagens bem-sucedidas
            successful_approaches = [
                exp for exp in experiences 
                if exp.get('success', False) and exp.get('quality', 0) >= 7.0
            ]
            
            if not successful_approaches:
                return None
            
            # Extrair template de abordagem
            best_approach = max(successful_approaches, key=lambda x: x.get('quality', 0))
            template = best_approach.get('code', '')[:200] + "..." if len(best_approach.get('code', '')) > 200 else best_approach.get('code', '')
            
            # Gerar descriÃ§Ã£o
            description = f"PadrÃ£o para tarefas de {domain}: abordagem bem-sucedida em {len(successful_approaches)}/{len(experiences)} casos"
            
            return DiscoveredPattern(
                id=f"task_pattern_{domain}",
                name=f"PadrÃ£o {domain.title()}",
                description=description,
                template=template,
                success_rate=success_rate,
                usage_count=len(experiences),
                contexts=[domain],
                quality_impact=avg_quality,
                discovery_date=datetime.now(),
                related_experiences=[exp.get('id', '') for exp in experiences],
                confidence_score=min(success_rate * (len(successful_approaches) / len(experiences)), 1.0)
            )
            
        except Exception as e:
            print(f"âš ï¸ Erro ao criar padrÃ£o de tarefa: {e}")
            return None
    
    def _analyze_quality_patterns(self, experiences: List[Dict]) -> List[DiscoveredPattern]:
        """Analisa padrÃµes que levam a alta qualidade"""
        patterns = []
        
        try:
            # Dividir em alta e baixa qualidade
            high_quality = [exp for exp in experiences if exp.get('quality', 0) >= 8.0]
            low_quality = [exp for exp in experiences if exp.get('quality', 0) < 6.0]
            
            if len(high_quality) < 3:
                return patterns
            
            # Analisar caracterÃ­sticas de alta qualidade
            hq_codes = [exp.get('code', '') for exp in high_quality]
            lq_codes = [exp.get('code', '') for exp in low_quality]
            
            # Encontrar elementos que aparecem mais em alta qualidade
            quality_indicators = self._find_quality_indicators(hq_codes, lq_codes)
            
            if quality_indicators:
                pattern = DiscoveredPattern(
                    id="quality_pattern_high",
                    name="PadrÃ£o de Alta Qualidade",
                    description=f"Elementos que aumentam qualidade: {', '.join(quality_indicators)}",
                    template=f"# Incluir: {', '.join(quality_indicators)}",
                    success_rate=1.0,
                    usage_count=len(high_quality),
                    contexts=["quality_improvement"],
                    quality_impact=sum(exp.get('quality', 0) for exp in high_quality) / len(high_quality),
                    discovery_date=datetime.now(),
                    related_experiences=[exp.get('id', '') for exp in high_quality],
                    confidence_score=len(high_quality) / len(experiences)
                )
                patterns.append(pattern)
                
        except Exception as e:
            print(f"âš ï¸ Erro na anÃ¡lise de padrÃµes de qualidade: {e}")
        
        return patterns
    
    def _find_quality_indicators(self, high_quality_codes: List[str], low_quality_codes: List[str]) -> List[str]:
        """Encontra elementos que indicam alta qualidade"""
        indicators = []
        
        # Elementos a verificar
        quality_elements = [
            ('docstrings', ['"""', "'''"]),
            ('error_handling', ['try:', 'except:', 'raise']),
            ('validation', ['if not', 'assert', 'validate']),
            ('comments', ['#']),
            ('type_hints', [': str', ': int', ': float', ': bool', '->'])
        ]
        
        hq_text = ' '.join(high_quality_codes).lower()
        lq_text = ' '.join(low_quality_codes).lower() if low_quality_codes else ''
        
        for element_name, keywords in quality_elements:
            hq_count = sum(hq_text.count(keyword) for keyword in keywords)
            lq_count = sum(lq_text.count(keyword) for keyword in keywords) if lq_text else 0
            
            # Se aparece mais em alta qualidade
            if hq_count > lq_count * 1.5:  # 50% mais frequente
                indicators.append(element_name)
        
        return indicators
    
    def _validate_patterns(self, patterns: List[DiscoveredPattern], 
                          min_occurrences: int, min_success_rate: float) -> List[DiscoveredPattern]:
        """Valida e filtra padrÃµes descobertos"""
        validated = []
        
        for pattern in patterns:
            if (pattern.usage_count >= min_occurrences and 
                pattern.success_rate >= min_success_rate and
                pattern.confidence_score >= 0.5):
                validated.append(pattern)
        
        # Ordenar por confianÃ§a
        validated.sort(key=lambda p: p.confidence_score, reverse=True)
        
        return validated
    
    def _update_pattern_evolution(self, new_patterns: List[DiscoveredPattern]):
        """Atualiza evoluÃ§Ã£o dos padrÃµes ao longo do tempo"""
        timestamp = datetime.now().isoformat()
        
        for pattern in new_patterns:
            if pattern.id not in self.pattern_evolution:
                self.pattern_evolution[pattern.id] = []
            
            self.pattern_evolution[pattern.id].append({
                'timestamp': timestamp,
                'usage_count': pattern.usage_count,
                'success_rate': pattern.success_rate,
                'quality_impact': pattern.quality_impact,
                'confidence_score': pattern.confidence_score
            })
    
    def _integrate_with_symbolic_system(self, patterns: List[DiscoveredPattern]):
        """
        CORRIGIDO: Integra padrÃµes descobertos com sistema simbÃ³lico atual
        Garante criaÃ§Ã£o de symbolic_traits e mapeamento robusto
        """
        try:
            # Carregar estado simbÃ³lico atual
            with open(IDENTITY_STATE, 'r', encoding='utf-8') as f:
                identity_data = yaml.safe_load(f) or {}
            
            # CORREÃ‡ÃƒO: Adicionar insights de padrÃµes aos agentes
            for agent_name in identity_data.keys():
                if agent_name not in identity_data:
                    continue
                
                agent_data = identity_data[agent_name]
                
                # CORREÃ‡ÃƒO: Garantir que symbolic_traits existe SEMPRE
                if 'symbolic_traits' not in agent_data:
                    agent_data['symbolic_traits'] = []
                
                # Adicionar padrÃµes relevantes
                relevant_patterns = [
                    p for p in patterns 
                    if p.success_rate > 0.8 and p.confidence_score > 0.7
                ]
                
                if relevant_patterns:
                    if 'discovered_patterns' not in agent_data:
                        agent_data['discovered_patterns'] = []
                    
                    for pattern in relevant_patterns[:3]:  # Top 3 padrÃµes
                        agent_data['discovered_patterns'].append({
                            'name': pattern.name,
                            'success_rate': pattern.success_rate,
                            'contexts': pattern.contexts,
                            'discovery_date': pattern.discovery_date.isoformat()
                        })
                    
                    # CORREÃ‡ÃƒO: Mapeamento robusto de padrÃµes para traits simbÃ³licos
                    new_traits = self._map_patterns_to_traits(relevant_patterns)
                    
                    # Adicionar novos traits Ãºnicos
                    existing_traits = set(agent_data.get('traits', []))
                    existing_symbolic_traits = set(agent_data.get('symbolic_traits', []))
                    
                    for trait in new_traits:
                        # Adicionar a traits normais se nÃ£o existir
                        if trait not in existing_traits:
                            if 'traits' not in agent_data:
                                agent_data['traits'] = []
                            agent_data['traits'].append(trait)
                            existing_traits.add(trait)
                        
                        # Adicionar a symbolic_traits se nÃ£o existir
                        if trait not in existing_symbolic_traits:
                            agent_data['symbolic_traits'].append(trait)
                            existing_symbolic_traits.add(trait)
            
            # Salvar estado atualizado
            with open(IDENTITY_STATE, 'w', encoding='utf-8') as f:
                yaml.safe_dump(identity_data, f, allow_unicode=True, sort_keys=False)
            
            print(f"ğŸ”— {len(patterns)} padrÃµes integrados ao sistema simbÃ³lico")
            
        except Exception as e:
            print(f"âš ï¸ Erro na integraÃ§Ã£o simbÃ³lica: {e}")
    
    def _map_patterns_to_traits(self, patterns: List[DiscoveredPattern]) -> List[str]:
        """
        NOVO: Mapeia padrÃµes descobertos para traits simbÃ³licos
        """
        traits = []
        
        for pattern in patterns:
            pattern_name_lower = pattern.name.lower()
            pattern_desc_lower = pattern.description.lower()
            contexts = [ctx.lower() for ctx in pattern.contexts]
            
            # Mapeamento baseado no nome do padrÃ£o
            if 'qualidade' in pattern_name_lower or 'quality' in pattern_name_lower:
                traits.append('Qualidade-Orientado')
            
            if 'validaÃ§Ã£o' in pattern_name_lower or 'validation' in pattern_name_lower:
                traits.append('Validador')
            
            if 'seguranÃ§a' in pattern_name_lower or 'security' in pattern_name_lower:
                traits.append('SeguranÃ§a-Consciente')
            
            if 'performance' in pattern_name_lower or 'otimizaÃ§Ã£o' in pattern_name_lower:
                traits.append('Performance-Otimizado')
            
            # Mapeamento baseado no contexto
            if 'authentication' in contexts:
                traits.append('Especialista-Auth')
            
            if 'api_development' in contexts:
                traits.append('API-Developer')
            
            if 'database' in contexts:
                traits.append('Data-Handler')
            
            if 'testing' in contexts:
                traits.append('Test-Driven')
            
            # Mapeamento baseado na descriÃ§Ã£o
            if 'error' in pattern_desc_lower or 'exception' in pattern_desc_lower:
                traits.append('Error-Handler')
            
            if 'docstring' in pattern_desc_lower or 'documentation' in pattern_desc_lower:
                traits.append('Documentador')
            
            # Mapeamento baseado na taxa de sucesso
            if pattern.success_rate >= 0.95:
                traits.append('Altamente-ConfiÃ¡vel')
            elif pattern.success_rate >= 0.85:
                traits.append('ConfiÃ¡vel')
            
            # Mapeamento baseado no impacto na qualidade
            if pattern.quality_impact >= 9.0:
                traits.append('Excellence-Seeker')
            elif pattern.quality_impact >= 8.0:
                traits.append('Quality-Focused')
        
        # Remover duplicatas e retornar
        return list(set(traits))
    
    def _extract_context(self, task: str) -> str:
        """Extrai contexto/domÃ­nio da tarefa"""
        task_lower = task.lower()
        
        if any(word in task_lower for word in ['login', 'auth', 'password', 'user']):
            return 'authentication'
        elif any(word in task_lower for word in ['api', 'endpoint', 'rest', 'http']):
            return 'api_development'
        elif any(word in task_lower for word in ['database', 'db', 'sql', 'query']):
            return 'database'
        elif any(word in task_lower for word in ['test', 'verify', 'check']):
            return 'testing'
        elif any(word in task_lower for word in ['file', 'read', 'write', 'csv']):
            return 'file_processing'
        elif any(word in task_lower for word in ['soma', 'calc', 'math', 'number']):
            return 'mathematical'
        else:
            return 'general'
    
    def _generate_pattern_description(self, template: str, contexts: List[str], 
                                    experiences: List[Dict]) -> str:
        """Gera descriÃ§Ã£o humana do padrÃ£o"""
        context_str = ', '.join(contexts) if contexts else 'geral'
        success_count = sum(1 for exp in experiences if exp.get('success', False))
        
        return f"PadrÃ£o comum para {context_str}: usado em {len(experiences)} casos, {success_count} sucessos"
    
    def get_pattern_recommendations(self, current_task: str) -> List[Dict[str, Any]]:
        """Retorna recomendaÃ§Ãµes de padrÃµes para tarefa atual"""
        if not self.discovered_patterns:
            return []
        
        task_context = self._extract_context(current_task)
        recommendations = []
        
        for pattern in self.discovered_patterns:
            if (task_context in pattern.contexts or 
                'general' in pattern.contexts or
                pattern.success_rate > 0.9):
                
                relevance_score = self._calculate_relevance(pattern, current_task, task_context)
                
                recommendations.append({
                    'pattern': pattern,
                    'relevance_score': relevance_score,
                    'recommendation': self._generate_recommendation_text(pattern, current_task)
                })
        
        # Ordenar por relevÃ¢ncia
        recommendations.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        return recommendations[:5]  # Top 5 recomendaÃ§Ãµes
    
    def _calculate_relevance(self, pattern: DiscoveredPattern, task: str, context: str) -> float:
        """Calcula relevÃ¢ncia do padrÃ£o para a tarefa"""
        score = 0.0
        
        # Contexto matching
        if context in pattern.contexts:
            score += 0.4
        
        # Success rate
        score += pattern.success_rate * 0.3
        
        # Quality impact
        score += (pattern.quality_impact / 10.0) * 0.2
        
        # Confidence
        score += pattern.confidence_score * 0.1
        
        return min(score, 1.0)
    
    def _generate_recommendation_text(self, pattern: DiscoveredPattern, task: str) -> str:
        """Gera texto de recomendaÃ§Ã£o para o padrÃ£o"""
        return f"Considere usar o padrÃ£o '{pattern.name}' (taxa de sucesso: {pattern.success_rate:.1%}, qualidade mÃ©dia: {pattern.quality_impact:.1f})"
    
    def export_patterns_summary(self) -> Dict[str, Any]:
        """Exporta resumo dos padrÃµes para dashboard"""
        return {
            'total_patterns': len(self.discovered_patterns),
            'patterns': [
                {
                    'id': p.id,
                    'name': p.name,
                    'description': p.description,
                    'success_rate': p.success_rate,
                    'usage_count': p.usage_count,
                    'quality_impact': p.quality_impact,
                    'contexts': p.contexts,
                    'confidence_score': p.confidence_score,
                    'discovery_date': p.discovery_date.isoformat()
                }
                for p in self.discovered_patterns
            ],
            'pattern_evolution': self.pattern_evolution,
            'summary_stats': {
                'avg_success_rate': sum(p.success_rate for p in self.discovered_patterns) / len(self.discovered_patterns) if self.discovered_patterns else 0,
                'total_experiences_analyzed': sum(p.usage_count for p in self.discovered_patterns),
                'high_confidence_patterns': len([p for p in self.discovered_patterns if p.confidence_score > 0.8])
            }
        }


# Scheduler para execuÃ§Ã£o automÃ¡tica (mantido inalterado)
import threading
import time

class PatternDiscoveryScheduler:
    """Scheduler para executar descoberta de padrÃµes automaticamente"""
    
    def __init__(self, discovery_engine: PatternDiscoveryEngine, 
                 interval_hours: int = 24):
        self.engine = discovery_engine
        self.interval_hours = interval_hours
        self.running = False
        self.thread = None
    
    def start(self):
        """Inicia execuÃ§Ã£o automÃ¡tica"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._run_discovery_loop)
            self.thread.daemon = True
            self.thread.start()
            print(f"ğŸ”„ Pattern Discovery agendado a cada {self.interval_hours}h")
    
    def stop(self):
        """Para execuÃ§Ã£o automÃ¡tica"""
        self.running = False
        if self.thread:
            self.thread.join()
        print("â¹ï¸ Pattern Discovery interrompido")
    
    def _run_discovery_loop(self):
        """Loop principal de descoberta"""
        while self.running:
            try:
                print("ğŸ• Executando descoberta automÃ¡tica de padrÃµes...")
                patterns = self.engine.discover_patterns()
                print(f"âœ… Descoberta concluÃ­da: {len(patterns)} padrÃµes")
                
            except Exception as e:
                print(f"âŒ Erro na descoberta automÃ¡tica: {e}")
            
            # Aguardar prÃ³xima execuÃ§Ã£o
            time.sleep(self.interval_hours * 3600)


# Exemplo de uso (mantido inalterado)
if __name__ == "__main__":
    # Inicializar sistema
    memory_store = HybridMemoryStore(enable_graphrag=True)
    discovery_engine = PatternDiscoveryEngine(memory_store)
    
    # Descobrir padrÃµes
    patterns = discovery_engine.discover_patterns()
    
    print(f"\nğŸ“Š RESUMO DOS PADRÃ•ES DESCOBERTOS:")
    print(f"Total: {len(patterns)}")
    
    for pattern in patterns:
        print(f"\nğŸ”¹ {pattern.name}")
        print(f"   DescriÃ§Ã£o: {pattern.description}")
        print(f"   Taxa de sucesso: {pattern.success_rate:.1%}")
        print(f"   Impacto na qualidade: {pattern.quality_impact:.1f}")
        print(f"   ConfianÃ§a: {pattern.confidence_score:.1%}")
    
    # Testar recomendaÃ§Ãµes
    print(f"\nğŸ’¡ RECOMENDAÃ‡Ã•ES PARA 'criar funÃ§Ã£o de login':")
    recommendations = discovery_engine.get_pattern_recommendations("criar funÃ§Ã£o de login")
    
    for rec in recommendations:
        print(f"   â€¢ {rec['recommendation']} (RelevÃ¢ncia: {rec['relevance_score']:.1%})")
    
    # Exportar resumo
    summary = discovery_engine.export_patterns_summary()
    print(f"\nğŸ“ˆ ESTATÃSTICAS:")
    print(f"   PadrÃµes de alta confianÃ§a: {summary['summary_stats']['high_confidence_patterns']}")
    print(f"   Taxa mÃ©dia de sucesso: {summary['summary_stats']['avg_success_rate']:.1%}")
    
    memory_store.close()