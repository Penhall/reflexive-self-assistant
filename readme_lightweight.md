# üöÄ RSCA com Modelos Leves - Guia R√°pido

## üéØ Objetivo

Configurar o **Reflexive Self Coding Assistant** para funcionar com modelos LLM leves (1B-2B par√¢metros) que rodam eficientemente em m√°quinas de desenvolvimento com RAM limitada.

## ‚ö° Setup R√°pido (5 minutos)

### 1. **Setup Autom√°tico**
```bash
# Setup completo em um comando
python quick_setup_lightweight.py
```

### 2. **Setup Manual** (se autom√°tico falhar)

#### Passo 1: Docker
```bash
# Iniciar containers
docker-compose up -d

# Aguardar Ollama inicializar (30-60s)
docker-compose logs ollama
```

#### Passo 2: Instalar Modelos
```bash
# Modelo ultra-leve para testes
ollama pull tinyllama:1.1b

# Modelo especializado em c√≥digo  
ollama pull codegemma:2b

# Modelo equilibrado
ollama pull phi3:mini
```

#### Passo 3: Testar Sistema
```bash
python test_lightweight_models.py
```

## üì¶ Modelos Recomendados

| Modelo | Tamanho | RAM | Velocidade | Especialidade |
|--------|---------|-----|------------|---------------|
| **TinyLlama 1.1B** | ~800MB | ~1GB | ‚ö° Muito r√°pido | Testes r√°pidos |
| **CodeGemma 2B** | ~1.5GB | ~2GB | üîÑ M√©dio | üíª **C√≥digo** |
| **Phi3 Mini** | ~2GB | ~2.5GB | üîÑ M√©dio | üìö Equilibrado |
| **Llama 3.2 1B** | ~1GB | ~1.5GB | ‚ö° R√°pido | Tarefas simples |

### üéØ Modelo Recomendado por Tarefa

```python
# Configura√ß√£o autom√°tica no sistema
TASK_MODELS = {
    "codigo": "codegemma:2b",      # Melhor para programa√ß√£o
    "testes": "tinyllama:1.1b",    # R√°pido para testes
    "documentacao": "phi3:mini",   # Equilibrado para texto
    "analise": "phi3:mini"         # An√°lise geral
}
```

## üß™ Testando o Sistema

### Teste Completo
```bash
python test_lightweight_models.py
```

### Teste R√°pido
```bash
# Verificar modelos instalados
ollama list

# Teste manual
curl http://localhost:11434/api/tags
```

### Exemplo de Uso
```python
from core.agents.code_agent import CodeAgent

# Criar agente com modelos leves
agent = CodeAgent(use_mock=False)

# Gerar c√≥digo (usa automaticamente CodeGemma 2B)
result = agent.execute_task("Criar fun√ß√£o que calcula fibonacci")

print(f"Sucesso: {result.success}")
print(f"Qualidade: {result.quality_score}/10")
print(f"C√≥digo:\n{result.code}")
```

## üîÑ Executando o Sistema Completo

### Modo Desenvolvimento
```bash
# Execu√ß√£o √∫nica
python core/main.py

# Dashboard visual
streamlit run interface/dashboard/streamlit_app.py

# Ciclos autom√°ticos
python scripts/scheduler.py
```

### Monitoramento
```bash
# Status dos containers
docker-compose ps

# Logs do Ollama
docker-compose logs -f ollama

# Uso de recursos
docker stats
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Otimiza√ß√£o por RAM Dispon√≠vel

#### üü¢ **2-4GB RAM Dispon√≠vel**
```python
# Use apenas TinyLlama
PREFERRED_MODEL = "tinyllama:1.1b"
MAX_CONCURRENT_MODELS = 1
```

#### üü° **4-6GB RAM Dispon√≠vel** 
```python
# CodeGemma para c√≥digo, TinyLlama para testes
MODELS = {
    "code": "codegemma:2b",
    "other": "tinyllama:1.1b"
}
```

#### üü¢ **6GB+ RAM Dispon√≠vel**
```python
# Configura√ß√£o completa recomendada
MODELS = {
    "code": "codegemma:2b",
    "docs": "phi3:mini", 
    "tests": "tinyllama:1.1b"
}
```

### Ajustes de Performance

```python
# config/lightweight_config.py
OLLAMA_CONFIG = {
    "timeout": 60,          # Timeout mais longo para modelos leves
    "retry_attempts": 2,    # Tentativas em caso de falha
    "temperature": 0.1,     # Menos criatividade, mais precis√£o
    "max_tokens": 1024,     # Respostas mais concisas
}
```

## üîß Solu√ß√£o de Problemas

### ‚ùå **Ollama n√£o conecta**
```bash
# Verificar se container est√° rodando
docker-compose ps

# Reiniciar se necess√°rio
docker-compose restart ollama

# Verificar logs
docker-compose logs ollama
```

### ‚ùå **Modelo n√£o encontrado**
```bash
# Listar modelos instalados
ollama list

# Instalar modelo espec√≠fico
ollama pull codegemma:2b

# Verificar se modelo funciona
ollama run codegemma:2b "def hello(): return"
```

### ‚ùå **Erro de RAM/Timeout**
```bash
# Verificar uso de RAM
free -h

# Matar processos que consomem RAM
docker stats
docker-compose down

# Usar apenas o modelo mais leve
ollama run tinyllama:1.1b
```

### ‚ùå **C√≥digo gerado com baixa qualidade**
```python
# Ajustar configura√ß√µes do modelo
GENERATION_CONFIG = {
    "temperature": 0.05,    # Mais determin√≠stico
    "top_p": 0.85,         # Menos aleatoriedade
    "repeat_penalty": 1.2  # Evitar repeti√ß√µes
}
```

### ‚ùå **Sistema lento**
```bash
# Usar modelo mais r√°pido
export PREFERRED_MODEL="tinyllama:1.1b"

# Reduzir tamanho das respostas
export MAX_TOKENS=512

# Verificar se h√° outros processos pesados
htop
```

## üìä M√©tricas de Performance

### Benchmarks Esperados

| Modelo | Tempo M√©dio | Qualidade | RAM Peak | Recomenda√ß√£o |
|--------|-------------|-----------|----------|--------------|
| TinyLlama 1.1B | ~3-5s | 6/10 | ~1GB | ‚úÖ Desenvolvimento r√°pido |
| CodeGemma 2B | ~8-12s | 8/10 | ~2GB | ‚úÖ **Melhor equil√≠brio** |
| Phi3 Mini | ~10-15s | 7.5/10 | ~2.5GB | ‚úÖ Tarefas mistas |

### Qualidade do C√≥digo Esperada

```
‚úÖ Sintaxe correta: >95%
‚úÖ Funcionalidade b√°sica: >85%
‚úÖ Boas pr√°ticas: >70%
‚úÖ Documenta√ß√£o: >60%
‚ö†Ô∏è Otimiza√ß√£o avan√ßada: ~40%
‚ö†Ô∏è Arquiteturas complexas: ~30%
```

## üéØ Casos de Uso Ideais

### ‚úÖ **Funciona Muito Bem**
- Fun√ß√µes simples e utilit√°rias
- Scripts de automa√ß√£o
- Testes unit√°rios b√°sicos
- Documenta√ß√£o de c√≥digo
- Prototipagem r√°pida
- Refatora√ß√£o simples

### ‚ö†Ô∏è **Funciona com Limita√ß√µes**
- APIs REST complexas
- Algoritmos avan√ßados
- Design patterns elaborados
- Otimiza√ß√µes de performance
- Arquiteturas de sistema

### ‚ùå **N√£o Recomendado**
- Sistemas distribu√≠dos complexos
- Machine Learning/IA avan√ßada
- Otimiza√ß√µes de baixo n√≠vel
- Arquiteturas enterprise
- C√≥digo cr√≠tico de produ√ß√£o sem revis√£o

## üîÆ Roadmap e Pr√≥ximos Passos

### Fase Atual: Modelos Leves Funcionais
- [x] Setup de modelos 1B-2B
- [x] Integra√ß√£o com CodeAgent
- [x] Testes automatizados
- [x] Dashboard b√°sico

### Pr√≥xima Fase: GraphRAG + Especializa√ß√£o
- [ ] Implementar GraphRAG para experi√™ncias
- [ ] Sistema de checkpoints de agentes
- [ ] Especializa√ß√£o autom√°tica por dom√≠nio
- [ ] Transfer learning entre projetos

### Fase Futura: Ecosystem de Agentes
- [ ] Reposit√≥rio de agentes especializados
- [ ] Agentes colaborativos
- [ ] Otimiza√ß√£o autom√°tica de performance
- [ ] Integra√ß√£o com IDEs

## üí° Dicas de Produtividade

### Para Desenvolvimento Di√°rio
```bash
# Alias √∫teis para .bashrc
alias rsca-start="docker-compose up -d && sleep 30"
alias rsca-test="python test_lightweight_models.py"
alias rsca-run="python core/main.py"
alias rsca-dash="streamlit run interface/dashboard/streamlit_app.py"
alias rsca-status="docker-compose ps && ollama list"
```

### Workflow Recomendado
1. **Manh√£**: `rsca-start` para inicializar
2. **Desenvolvimento**: Use CodeGemma 2B para c√≥digo principal
3. **Testes**: TinyLlama para valida√ß√£o r√°pida
4. **Documenta√ß√£o**: Phi3 Mini para textos
5. **Fim do dia**: `docker-compose down` para economizar recursos

### Integra√ß√£o com IDE
```python
# .vscode/tasks.json
{
    "version": "2.0.0",
    "tasks": [
        {
            "label": "RSCA Generate Code",
            "type": "shell",
            "command": "python -c \"from core.agents.code_agent import CodeAgent; agent = CodeAgent(); result = agent.execute_task('${input:task}'); print(result.code)\""
        }
    ]
}
```

## üìö Recursos Adicionais

### Documenta√ß√£o
- [Arquitetura Geral](docs/ARCHITECTURE.md)
- [Guia de Performance](docs/PERFORMANCE.md) 
- [API Reference](docs/API_REFERENCE.md)
- [Solu√ß√£o de Problemas](docs/TROUBLESHOOTING.md)

### Comunidade
- **Issues**: Reporte problemas espec√≠ficos com modelos leves
- **Discussions**: Compartilhe configura√ß√µes otimizadas
- **Wiki**: Casos de uso e exemplos da comunidade

### Monitoramento
```bash
# Script de monitoramento cont√≠nuo
watch -n 5 'echo "=== RSCA Status ===" && docker-compose ps && echo && ollama list && echo && free -h'
```

## ‚ö° Quick Reference

### Comandos Essenciais
```bash
# Setup completo
python quick_setup_lightweight.py

# Teste r√°pido
python test_lightweight_models.py

# Executar sistema
python core/main.py

# Dashboard
streamlit run interface/dashboard/streamlit_app.py

# Status
docker-compose ps && ollama list
```

### Modelos por Situa√ß√£o
- **üöÄ Pressa/RAM baixa**: `tinyllama:1.1b`
- **üíª C√≥digo importante**: `codegemma:2b`  
- **üìö Texto/Docs**: `phi3:mini`
- **üî¨ Experimenta√ß√£o**: `llama3.2:1b`

---

## üéâ Pronto para Come√ßar!

Se seguiu este guia, seu RSCA deve estar funcionando com modelos leves e otimizado para desenvolvimento. 

**Pr√≥ximo passo**: Execute `python test_lightweight_models.py` e veja a m√°gica acontecer! ü™Ñ